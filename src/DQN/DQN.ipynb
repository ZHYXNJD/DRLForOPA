{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "from OPA import OPA\n",
    "import static_data as sd\n",
    "\n",
    "total_slot = sd.total_slot              # 泊位总数量\n",
    "park_slot_index = sd.ops_index          # 普通泊位索引\n",
    "charge_slot_index = sd.cps_index        # 充电桩索引\n",
    "slot_index = [park_slot_index,charge_slot_index]   # 两类索引集合\n",
    "window_time = sd.window_time            # 将时间离散化后的时间间隔总数 这里是195（15min为单位）\n",
    "total_request = 2120                    # 2000个普通请求+120个充电请求\n",
    "\n",
    "req_info = pd.read_csv(sd.req_info_path)\n",
    "req_revenue = np.array((req_info['parking_t'].fillna(0)+req_info['char_t'].fillna(0)).values,dtype=int)\n",
    "req_type = np.array(req_info[\"charge_label\"],dtype=int)\n",
    "rmk = np.array(pd.read_csv(sd.r_mk_path))\n",
    "\n",
    "# 即时决策的话\n",
    "# 停车场泊位供应状态 + 一个需求信息 + 需求种类\n",
    "state_size = (total_slot + 1) * window_time + 1\n",
    "# 动作空间为停车泊位的索引\n",
    "action_size = total_slot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size,action_size=action_size,seed=0)\n",
    "env = OPA()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsqueeze(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 41>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     38\u001B[0m            torch\u001B[38;5;241m.\u001B[39msave(agent\u001B[38;5;241m.\u001B[39mqnetwork_local\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcheckpoint.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     39\u001B[0m            \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mdqn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# plot the scores\u001B[39;00m\n\u001B[0;32m     44\u001B[0m fig \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mfigure()\n",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36mdqn\u001B[1;34m(n_episode, episode_length, eps_start, eps_end, eps_decay)\u001B[0m\n\u001B[0;32m     22\u001B[0m score \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(episode_length):\n\u001B[1;32m---> 24\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43magent_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m     next_state,reward,cum_rewards,done \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     26\u001B[0m     copy_next_state \u001B[38;5;241m=\u001B[39m deepcopy(next_state)\n",
      "File \u001B[1;32mG:\\纵向\\小论文\\停车分配\\DRL\\src\\DQN\\dqn_agent.py:112\u001B[0m, in \u001B[0;36mAgent.act\u001B[1;34m(self, state, eps)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;124;03mParams\u001B[39;00m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;124;03m======\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;124;03m:return: return actions for given state per policy\u001B[39;00m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;66;03m# state = torch.from_numpy(state).float().unsqueeze(0).to(device)\u001B[39;00m\n\u001B[1;32m--> 112\u001B[0m state \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqnetwork_local\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "\u001B[1;31mTypeError\u001B[0m: unsqueeze(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def new_state(env_state):\n",
    "    return torch.concat([torch.tensor(env_state[\"supply\"].flatten()),torch.tensor(env_state[\"demand\"].flatten()),torch.tensor(env_state[\"type\"].flatten())])\n",
    "\n",
    "def dqn(n_episode=2000,episode_length=total_request,eps_start=1.0,eps_end=0.01,eps_decay=0.995):\n",
    "    \"\"\"\n",
    "\n",
    "    :param n_episode: max number of training episodes\n",
    "    :param episode_length:\n",
    "    :param eps_start:\n",
    "    :param eps_end:\n",
    "    :param eps_decay:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1,n_episode+1):\n",
    "        state = env.reset()\n",
    "        copy_state = deepcopy(state)  # 因为是字典 需要深拷贝 否则会修改原state  这个state仍然是字典 可以通过关键字得到对应的值\n",
    "        agent_state = new_state(copy_state)\n",
    "        score = 0\n",
    "        for t in range(episode_length):\n",
    "            action = agent.act(agent_state,eps)\n",
    "            next_state,reward,cum_rewards,done = env.step(action)\n",
    "            copy_next_state = deepcopy(next_state)\n",
    "            agent_next_state = new_state(copy_next_state)\n",
    "            agent.step(agent_state,action,reward,agent_next_state,done)\n",
    "            agent_state = agent_next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end,eps_decay*eps)\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\t Score: {:.2f}'.format(i_episode,score), end=\"\")\n",
    "        if i_episode % 20 == 0:\n",
    "           torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "           break\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}